# This workflow runs a locallyâ€‘hosted LLM inside the runner to generate critical comments on pull requests
# Based on: https://remarkablemark.org/blog/2025/02/23/run-ollama-large-language-models-on-github-actions/
name: A very stupid code review

on:
  pull_request:
    branches:
      - main

permissions:
  contents: read
  pull-requests: write

env:
  MODEL: kirito1/qwen3-coder:4b
  BOT_NAME: Stupid bot
  PROMPT: |
    Please review the code.

    Consider:
    1. Code quality and adherence to best practices
    2. Potential bugs or edge cases
    3. Performance optimizations
    4. Readability and maintainability
    5. Any security concerns

    Suggest improvements and explain your reasoning for each suggestion.

    The code is following:

jobs:
  code-review:
    runs-on: ubuntu-latest
    steps:
      - name: Restore the model cache
        uses: actions/cache@v4
        with:
          path: ~/.ollama
          key: ${{ runner.os }}-${{ env.MODEL }}
          restore-keys: |
            ${{ runner.os }}-${{ env.MODEL }}

      # Install ollama
      - name: Setup ollama
        uses: ai-action/setup-ollama@v1

      # Pull the model if it hasn't been cached yet
      - name: Download the model
        run: ollama pull ${{ env.MODEL }}

      # Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # Run code review and comment on PR
      - name: Code review comment
        run: |
          RESPONSE=$(ollama run ${{ env.MODEL }} "${{ env.PROMPT }}\n$(gh pr diff $PR_NUMBER)")
          gh pr comment $PR_NUMBER --body "$RESPONSE"
        env:
          GH_TOKEN: ${{ github.token }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
